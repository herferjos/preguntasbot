# -*- coding: utf-8 -*-
"""Q&A pdfs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TVItBIY4VR6qdb1ZTbADrWxEZjOi_fAc

pip install pypdf
import pypdf

pip install tiktoken
import tiktoken

pip install openai
import openai

import os
import re
import pandas as pd

pip install streamlit
import streamlit as st

import numpy as np
from openai.embeddings_utils import distances_from_embeddings
"""

import pypdf

import tiktoken

import openai

import os
import re
import pandas as pd

import streamlit as st

import numpy as np
from openai.embeddings_utils import distances_from_embeddings

def embededor(patrones_str, paginas_eliminar, carpeta):

  # Obtener la lista de archivos PDF en la carpeta especificada
  pdf_files = [f for f in os.listdir(carpeta) if f.endswith('.pdf')]

  # Crear una lista para almacenar los textos extra칤dos
  texts = []

  # Convertir los n칰meros de p치gina ingresados por el usuario en una lista
  paginas_eliminar = [int(pagina) for pagina in paginas_eliminar.split(",")]

  patrones = patrones_str.split(",")

  # Calcular el n칰mero de patrones
  num_patrones = len(patrones)

  # Recorrer los archivos PDF y convertirlos a texto usando pypdf
  for file in pdf_files:

      with open(os.path.join(carpeta, file), 'rb') as pdf_file:
          # Abrir el archivo PDF en modo binario y crear un objeto PdfReader
          pdf = pypdf.PdfReader(pdf_file)

          # Crear una variable para almacenar el texto extra칤do de cada p치gina del archivo PDF
          text = ''

          # Recorrer las p치ginas del archivo PDF y extraer el texto usando el m칠todo extract_text()
          for i, page in enumerate(pdf.pages):
              if i+1 in paginas_eliminar:
                  continue
              text += page.extract_text()

          # Eliminar los patrones del texto usando expresiones regulares
          for patron in patrones:
              pattern = re.compile(patron)
              text = re.sub(pattern, '', text)
          
          # A침adir el nombre del archivo y el texto extra칤do a la lista texts
          texts.append((file[:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))

  # Crear un dataframe con las columnas fname (nombre del archivo) y text (texto extra칤do)
  df = pd.DataFrame(texts, columns=['fname', 'text'])

  # Guardar el dataframe como un archivo CSV en la misma carpeta que los archivos PDF originales
  df.to_csv(os.path.join(carpeta, 'texts.csv'))

  # Load the cl100k_base tokenizer which is designed to work with the ada-002 model
  tokenizer = tiktoken.get_encoding("cl100k_base")

  df = pd.read_csv(os.path.join(carpeta, 'texts.csv'))
  df.columns = ['title', 'text']

  # Tokenize the text and save the number of tokens to a new column
  df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))


  max_tokens = 500

  # Function to split the text into chunks of a maximum number of tokens
  def split_into_many(text, max_tokens = max_tokens):

      # Split the text into sentences
      sentences = text.split('. ')

      # Get the number of tokens for each sentence
      n_tokens = [len(tokenizer.encode(" " + sentence)) for sentence in sentences]
      
      chunks = []
      tokens_so_far = 0
      chunk = []

      # Loop through the sentences and tokens joined together in a tuple
      for sentence, token in zip(sentences, n_tokens):

          # If the number of tokens so far plus the number of tokens in the current sentence is greater 
          # than the max number of tokens, then add the chunk to the list of chunks and reset
          # the chunk and tokens so far
          if tokens_so_far + token > max_tokens:
              chunks.append(". ".join(chunk) + ".")
              chunk = []
              tokens_so_far = 0

          # If the number of tokens in the current sentence is greater than the max number of 
          # tokens, go to the next sentence
          if token > max_tokens:
              continue

          # Otherwise, add the sentence to the chunk and add the number of tokens to the total
          chunk.append(sentence)
          tokens_so_far += token + 1

      return chunks

  shortened = []

  # Loop through the dataframe
  for row in df.iterrows():

      # If the text is None, go to the next row
      if row[1]['text'] is None:
          continue

      # If the number of tokens is greater than the max number of tokens, split the text into chunks
      if row[1]['n_tokens'] > max_tokens:
          shortened += split_into_many(row[1]['text'])
      
      # Otherwise, add the text to the list of shortened texts
      else:
          shortened.append( row[1]['text'] )

  df = pd.DataFrame(shortened, columns = ['text'])
  df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))


  df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])

  df.to_csv(os.path.join(carpeta, 'embeddings.csv'))

  return df

def create_context(
    question, df, max_len=1800, size="ada"
):
    """
    Create a context for a question by finding the most similar context from the dataframe
    """

    # Get the embeddings for the question
    q_embeddings = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']

    # Get the distances from the embeddings
    df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')


    returns = []
    cur_len = 0

    # Sort by distance and add the text to the context until the context is too long
    for i, row in df.sort_values('distances', ascending=True).iterrows():
        
        # Add the length of the text to the current length
        cur_len += row['n_tokens'] + 4
        
        # If the context is too long, break
        if cur_len > max_len:
            break
        
        # Else add it to the text that is being returned
        returns.append(row["text"])

    # Return the context
    return "\n\n###\n\n".join(returns)

def answer_question(
    df,
    model="gpt-3.5-turbo",
    question="pregunta",
    max_len=1800,
    size="ada",
    debug=False,
    max_tokens=150,
    stop_sequence=None
):
    """
    Answer a question based on the most similar context from the dataframe texts
    """
    context = create_context(
        question,
        df,
        max_len=max_len,
        size=size,
    )
    # If debug, print the raw model response
    if debug:
        print("Contexto:\n" + context)
        print("\n\n")
    
    try:
        system="Eres un experto en responder preguntas bas치ndose en el contexto y no alucinando con la informaci칩n. Sea honesto y preciso"
        prompt=f"Responde a la pregunta bas치ndote en el contexto que aparece a continuaci칩n, selecciona una opci칩n y escribe s칩lo la opci칩n sin m치s texto, y si la pregunta no se puede responder bas치ndote en el contexto, di \"No lo s칠\"\n\nContexto: {context}\n\n---\n\nPregunta: {question}\nRespuesta:"
        conversation = [
            {"role": "system", "content": system}, {"role": "user", "content": prompt}
        ]
        
        # Create a completions using the question and context
        response = openai.ChatCompletion.create(
            messages = conversation,
            temperature=0,
            max_tokens=max_tokens,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0,
            stop=stop_sequence,
            model=model,
        )
        return response['choices'][0]['message']['content']
    except Exception as e:
        print(e)
        return ""

st.title("游 Prueba 游뱄 ")

user_secret = st.text_input(label = ":blue[OpenAI API key]",
                                placeholder = "Copia aqu칤 tu clave de OpenAI",
                                type = "password")
if user_secret:
    openai.api_key = user_secret

    opciones1 = ["PDFs","Webs"]
    opciones2 = ["Subir PDFs","Subir los PDFs indexados"]

    eleccion1 = st.selectbox("Selecciona una opci칩n", opciones1)

    if eleccion1 == "PDFs":
      eleccion2 = st.selectbox("Selecciona una opci칩n", opciones2)
      if eleccion2 == "Subir PDFs":
        # Creamos la carpeta "data" si no existe previamente
        if not os.path.exists("data"):
            os.makedirs("data")

        uploaded_files = st.file_uploader("Selecciona los archivos PDF", type="pdf", accept_multiple_files=True)

        if uploaded_files is not None:
            for uploaded_file in uploaded_files:
                file_path = os.path.join("data", uploaded_file.name)
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                st.write("Archivo guardado:", uploaded_file.name)
                
        st.write("쯈uieres eliminar alg칰n patr칩n de texto en tus archivos, que ensucie el texto de tus archivos? Por ejemplo: marcas de agua o encabezados")
        eleccion3 = st.checkbox("Eliminar patrones")
        st.write("쯈uieres eliminar algunas de las p치ginas de tus archivos? Por ejemplo: la portada o el 칤ndice")
        eleccion4 = st.checkbox("쯈uieres eliminar algunas de las p치ginas de tus archivos?")
        if eleccion3 == True:
            patrones_str = st.text_input("Ingrese los patrones separados por comas: ")
        if eleccion4 == True:
            paginas_eliminar = st.text_input("Escribe los n칰meros de p치gina separados por comas a eliminar (por ejemplo, 1,2,5): ")

        carpeta = os.path.abspath("data")    
        archivo_df = embededor(patrones_str, paginas_eliminar, carpeta)
        st.write("Guarda este archivo .csv que contiene el texto de los archivos pdfs indexados. 칔salo cada vez que abras la aplicaci칩n para buscar nuevamente")
        if st.button('Descargar archivo'):
          csv = archivo_df.to_csv(index=False)
          b64 = base64.b64encode(csv.encode()).decode()  # Codificar a base64
          href = f'data:text/csv;base64,{b64}'  # Crear el enlace para descargar el archivo
          st.markdown(f'<a href="{href}" download="data.csv">Descargar archivo</a>', unsafe_allow_html=True)
        
        archivo_df['embeddings'] = archivo_df['embeddings'].apply(eval).apply(np.array)


        pregunta = st.text_input(
            label=":blue[Pregunta lo que quieras]",
            placeholder="Por favor, responde a la pregunta..."
        )
        
        if pregunta:
            # And if they have clicked the search button
            if st.button(label="Buscar", type='primary'):
                # Run the search function and get the results
                st.write(answer_question(archivo_df, pregunta))

      else:
        csv_file = st.file_uploader("Selecciona el archivo .csv con el texto indexado", type="csv")

        if csv_file is not None:
          archivo_df = pd.read_csv(csv_file)
        
        archivo_df['embeddings'] = archivo_df['embeddings'].apply(eval).apply(np.array)

        pregunta = st.text_input(
            label=":blue[Pregunta lo que quieras]",
            placeholder="Por favor, responde a la pregunta..."
        )
        
        if pregunta:
            # And if they have clicked the search button
            if st.button(label="Buscar", type='primary'):
                # Run the search function and get the results
                st.write(answer_question(archivo_df, pregunta))